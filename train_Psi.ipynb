{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Neural ODE with the analytical $\\Psi$ from Govindjee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vt/miniforge3/envs/jax/lib/python3.10/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
      "  warnings.warn('jax.experimental.optimizers is deprecated, '\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from NODE_fns import sigma_split_vmap, sigma_biaxial_vmap\n",
    "from jax import grad, random, jit, vmap\n",
    "from functools import partial\n",
    "from jax.experimental import optimizers\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "key = random.PRNGKey(0)\n",
    "from jax.config import config\n",
    "config.update('jax_disable_jit', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "def init_params(layers, key):\n",
    "    Ws = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        std_glorot = np.sqrt(2/(layers[i] + layers[i + 1]))\n",
    "        key, subkey = random.split(key)\n",
    "        Ws.append(random.normal(subkey, (layers[i], layers[i + 1]))*std_glorot)\n",
    "    return Ws\n",
    "\n",
    "layers = [1, 5, 5, 1]\n",
    "I1_params = init_params(layers, key)\n",
    "I2_params = init_params(layers, key)\n",
    "J1_params = init_params(layers, key)\n",
    "alpha = 1.0\n",
    "Psi1_bias = -3.0\n",
    "Psi2_bias = -3.0\n",
    "NN_weights = (I1_params, I2_params, J1_params)\n",
    "params = (NN_weights, alpha, Psi1_bias, Psi2_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, lmb, sigma_gt):\n",
    "    lm1 = lmb[:,0]\n",
    "    lm2 = lmb[:,1]\n",
    "    lm3 = lmb[:,2]\n",
    "    sigma_pr = sigma_split_vmap(lm1, lm2, lm3, params)\n",
    "    loss1 = np.average((sigma_pr[:,0,0]-sigma_gt[:,0,0])**2) \n",
    "    loss2 = np.average((sigma_pr[:,1,1]-sigma_gt[:,1,1])**2)\n",
    "    loss = (loss1+loss2)/2\n",
    "    return  loss\n",
    "\n",
    "@partial(jit, static_argnums=(0,))\n",
    "def step(loss, i, opt_state, X_batch, Y_batch):\n",
    "    params = get_params(opt_state)\n",
    "    g = grad(loss)(params, X_batch, Y_batch)\n",
    "    return opt_update(i, g, opt_state)\n",
    "\n",
    "def train(loss, X, Y, opt_state, key, nIter = 10000, batch_size = 10):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for it in range(nIter):\n",
    "        key, subkey = random.split(key)\n",
    "        idx_batch = random.choice(subkey, X.shape[0], shape = (batch_size,), replace = False)\n",
    "        opt_state = step(loss, it, opt_state, X[idx_batch], Y[idx_batch])         \n",
    "        if (it+1)% 10000 == 0:\n",
    "            params = get_params(opt_state)\n",
    "            train_loss_value = loss(params, X, Y)\n",
    "            train_loss.append(train_loss_value)\n",
    "            to_print = \"it %i, train loss = %e\" % (it+1, train_loss_value)\n",
    "            print(to_print)\n",
    "    return get_params(opt_state), train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input data\n",
    "lm = np.linspace(0.9,1.2,20)\n",
    "lm1, lm2 = np.array(np.meshgrid(lm, lm))\n",
    "lm1 = lm1.reshape(-1)\n",
    "lm2 = lm2.reshape(-1)\n",
    "lm3 = 1/(lm1*lm2)\n",
    "lmb = np.transpose(np.array([lm1, lm2, lm3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output data\n",
    "mu = 77.77 #=shear_mod\n",
    "K = 10000\n",
    "sigma_gt = []\n",
    "for i in range(lm1.shape[0]):\n",
    "    b = np.array([[lm1[i]**2, 0, 0],\n",
    "                  [0, lm2[i]**2, 0],\n",
    "                  [0, 0, lm3[i]**2]])\n",
    "    J = lm1[i]*lm2[i]*lm3[i]\n",
    "    sigma_gt.append(mu/J*(b-np.eye(3)) + 2*K*(J-1)*np.eye(3))\n",
    "sigma_gt = np.stack(sigma_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "opt_init, opt_update, get_params = optimizers.adam(1.e-5)\n",
    "opt_state = opt_init(params)\n",
    "params_Psi_eq, train_loss, val_loss = train(loss, lmb, sigma_gt, opt_state, key, nIter = 100000, batch_size = 10)\n",
    "with open('saved/Psi_eq_params.npy', 'wb') as f:\n",
    "    pickle.dump(params_Psi_eq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the newly trained Neural ODEs\n",
    "sigma_pr = sigma_split_vmap(lm1, lm2, lm3, params)\n",
    "xmin,xmax = np.min(sigma_gt[:,0,0]), np.max(sigma_gt[:,0,0])\n",
    "ymin,ymax = np.min(sigma_gt[:,1,1]), np.max(sigma_gt[:,1,1])\n",
    "zmin,zmax = np.min(sigma_gt[:,2,2]), np.max(sigma_gt[:,2,2])\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=[10,4])\n",
    "ax[0].plot([xmin, xmax], [xmin, xmax], 'r-')\n",
    "ax[0].plot(sigma_gt[:,0,0],sigma_pr[:,0,0],'.')\n",
    "ax[1].plot([ymin, ymax], [ymin, ymax], 'r-')\n",
    "ax[1].plot(sigma_gt[:,1,1],sigma_pr[:,1,1],'.')\n",
    "\n",
    "fig.suptitle(r\"Results of training $\\Psi_{EQ}^{NODE}$ with the last point of each stress-stretch curve\")\n",
    "ax[0].set(xlabel=\"Ground truth $\\sigma_x$\", ylabel='Predicted $\\sigma_x$', aspect='equal', xlim=[xmin, xmax], ylim=[xmin, xmax])\n",
    "ax[1].set(xlabel=\"Ground truth $\\sigma_y$\", ylabel='Predicted $\\sigma_y$', aspect='equal', xlim=[ymin, ymax], ylim=[ymin, ymax])\n",
    "fig.savefig('figs/train_Psi_eq.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train $\\Psi_{NEQ}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_m = np.array([51.4, -18, 3.86])\n",
    "alpha_m = np.array([1.8, -2, 7])\n",
    "K_m = 10000\n",
    "def sigma_NEQ_gov(lm1, lm2, lm3):\n",
    "    J = lm1*lm2*lm3\n",
    "\n",
    "    lm1 = J**(-1/3)*lm1\n",
    "    lm2 = J**(-1/3)*lm2\n",
    "    lm3 = J**(-1/3)*lm3\n",
    "    sigma11 = 0\n",
    "    sigma22 = 0\n",
    "    sigma33 = 0\n",
    "    for i in range(3):\n",
    "        sigma11+= mu_m[i]*lm1**(alpha_m[i])\n",
    "        sigma22+= mu_m[i]*lm2**(alpha_m[i])\n",
    "        sigma33+= mu_m[i]*lm3**(alpha_m[i])\n",
    "    sigma = np.array([[sigma11, 0, 0],\n",
    "                      [0, sigma22, 0],\n",
    "                      [0, 0, sigma33]])\n",
    "    sigma = sigma/J\n",
    "\n",
    "    sigma_vol = K_m/2*(J**2-1)*np.eye(3)\n",
    "    sigma = sigma + sigma_vol\n",
    "    return sigma\n",
    "sigma_NEQ_vmap = vmap(sigma_NEQ_gov, in_axes=(0,0,0), out_axes=0)\n",
    "\n",
    "sigma_gt = sigma_NEQ_vmap(lm1, lm2, lm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = optimizers.adam(2.e-5)\n",
    "opt_state = opt_init(params)\n",
    "params_Psi_neq, train_loss, val_loss = train(loss, lmb, sigma_gt, opt_state, key, nIter = 100000, batch_size = 10)\n",
    "with open('saved/Psi_neq_params.npy', 'wb') as f:\n",
    "    pickle.dump(params_Psi_neq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_pr = sigma_split_vmap(lm1, lm2, lm3, params)\n",
    "xmin,xmax = np.min(sigma_gt[:,0,0]), np.max(sigma_gt[:,0,0])\n",
    "ymin,ymax = np.min(sigma_gt[:,1,1]), np.max(sigma_gt[:,1,1])\n",
    "zmin,zmax = np.min(sigma_gt[:,2,2]), np.max(sigma_gt[:,2,2])\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=[10,4])\n",
    "ax[0].plot([xmin, xmax], [xmin, xmax], 'r-')\n",
    "ax[0].plot(sigma_gt[:,0,0],sigma_pr[:,0,0],'.')\n",
    "ax[1].plot([ymin, ymax], [ymin, ymax], 'r-')\n",
    "ax[1].plot(sigma_gt[:,1,1],sigma_pr[:,1,1],'.')\n",
    "\n",
    "fig.suptitle(r\"Results of training $\\Psi_{NEQ}^{NODE}$ with the last point of each stress-stretch curve\")\n",
    "ax[0].set(xlabel=\"Ground truth $\\sigma_x$\", ylabel='Predicted $\\sigma_x$', aspect='equal', xlim=[xmin, xmax], ylim=[xmin, xmax])\n",
    "ax[1].set(xlabel=\"Ground truth $\\sigma_y$\", ylabel='Predicted $\\sigma_y$', aspect='equal', xlim=[ymin, ymax], ylim=[ymin, ymax])\n",
    "fig.savefig('figs/train_Psi_neq.jpg')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "920e7a8945330110f9e03acbda53cbdd3d881ab4a4172d15e426a9c275db3432"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
